Changeset created on Fri May  7 00:17:07 CEST 2010 by Seek You Too

Description: Fieldname wildcard support for Drilldown.

    Drilldown will now support fieldnames with wildcards with the from 'prefix.*'
    Every field added that starts with 'prefix.' will be added to the drilldown
    index. This feature is build for the project "EduRep" of KennisNet to
    support dynamicly created drilldown fields.

    This changeset fixes a Drilldown so tokenized fields are handled instantly, and
    a reread of the index is no longer necessary.

    This changeset has been recreated for 3.0-beta and is based on 201005062251.drilldown-fieldnames-wildcard.changeset

Baseline version: meresco-components/workingsets/3.0-beta/version_11

diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/meresco/components/facetindex/document.py version_12/meresco/components/facetindex/document.py
--- version_11/meresco/components/facetindex/document.py	2010-05-06 22:18:15.000000000 +0200
+++ version_12/meresco/components/facetindex/document.py	2010-05-07 00:16:59.000000000 +0200
@@ -9,6 +9,7 @@
 #    Copyright (C) 2009-2010 Delft University of Technology http://www.tudelft.nl
 #    Copyright (C) 2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2007-2010 Seek You Too (CQ2) http://www.cq2.nl
+#    Copyright (C) 2010 Stichting Kennisnet http://www.kennisnet.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -28,10 +29,22 @@
 #
 ## end license ##
 
-from merescolucene import Document as LuceneDocument, Field, Fieldable, iterJ, asFloat
+from collections import defaultdict
+from merescolucene import Document as LuceneDocument, Field, Fieldable, iterJ, asFloat, merescoStandardAnalyzer
+
+from java.io import StringReader, Reader
 
 IDFIELD = '__id__'
 
+def tokenize(aString):
+    ts = merescoStandardAnalyzer.tokenStream('ignored fieldname', StringReader(unicode(aString)) % Reader)
+    tokens = []
+    token = ts.next()
+    while token != None:
+        tokens.append(token.termText())
+        token = ts.next()
+    return tokens
+
 class DocumentException(Exception):
     """Generic Document Exception"""
     pass
@@ -42,17 +55,14 @@
         self.identifier = anId
         if not self._isValidFieldValue(anId):
             raise DocumentException("Invalid ID: '%s'" % anId)
-
-        self._document = LuceneDocument()
-        self._document.add(Field(IDFIELD, anId, Field.Store.YES, Field.Index.UN_TOKENIZED) % Fieldable)
-        self._fields = [IDFIELD]
-        self._tokenizedFields = []
+        self._fields = []
+        self._tokenize = tokenize
 
     def _isValidFieldValue(self, anObject):
         return isinstance(anObject, basestring) and anObject.strip()
 
     def fields(self):
-        return self._fields
+        return [IDFIELD] + [key for key, value, tokenize in self._fields]
 
     def _validFieldName(self, aKey):
         return self._isValidFieldValue(aKey) and aKey.lower() != IDFIELD
@@ -64,37 +74,61 @@
         if not self._isValidFieldValue(aValue):
             return
 
-        self._addIndexedField(aKey, aValue, tokenize)
-        self._fields.append(aKey)
-        if tokenize and not aKey in self._tokenizedFields:
-            self._tokenizedFields.append(aKey)
-
-    def _addIndexedField(self, aKey, aValue, tokenize = True):
-        self._document.add(Field(aKey,
-                                 aValue, 
-                                 Field.Store.NO,
-                                 tokenize and Field.Index.TOKENIZED or Field.Index.UN_TOKENIZED
-                           ) % Fieldable)
+        self._fields.append((aKey, aValue, tokenize))
 
     def addToIndexWith(self, anIndexWriter):
-        anIndexWriter.addDocument(self._document)
+        document = LuceneDocument()
+        document.add(Field(IDFIELD, self.identifier, Field.Store.YES, Field.Index.UN_TOKENIZED) % Fieldable)
+        for key, value, tokenize in self._fields:
+            document.add(Field(
+                    key,
+                    value, 
+                    Field.Store.NO,
+                    Field.Index.TOKENIZED if tokenize else Field.Index.UN_TOKENIZED
+                ) % Fieldable)
+        anIndexWriter.addDocument(document)
 
     def validate(self):
-        if self._fields == [IDFIELD]:
+        if self._fields == []:
             raise DocumentException('Empty document')
 
     def asDict(self):
-        dictionary = {}
-        for field in iterJ(self._document.getFields()):
-            key = field.name()
-            if not key in dictionary:
-                dictionary[key] = []
-            if not field.stringValue()  in dictionary[key]:
-                dictionary[key].append(field.stringValue())
-        return dictionary
+        return DocDict(self.fields, self._valuesForField)
 
-    def tokenizedFields(self):
-        return self._tokenizedFields
-
-    def __repr__(self):
-        return repr(self.asDict())
+    def _valuesForField(self, key):
+        if key == IDFIELD:
+            yield self.identifier
+            return
+        for fieldname, value, tokenize in self._fields:
+            if fieldname == key:
+                if tokenize:
+                    for v in self._tokenize(value):
+                        yield v
+                else:
+                    yield value
+
+def unique(iterable):
+    seen = set()
+    for item in iterable:
+        if item not in seen:
+            seen.add(item)
+            yield item
+            
+
+class DocDict(object):
+    def __init__(self, keysMethod, valuesMethod):
+        self._valuesMethod = lambda key: unique(valuesMethod(key))
+        self.keys = lambda: set(keysMethod())
+
+    def __cmp__(self, other):
+        return cmp(dict(self.items()), other)
+
+    def __getitem__(self, key, default=None):
+        if not key in self.keys():
+            return default
+        return list(self._valuesMethod(key))
+    get = __getitem__
+
+    def items(self):
+        return ((key, self[key]) for key in self.keys()) 
+ 
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/meresco/components/facetindex/drilldown.py version_12/meresco/components/facetindex/drilldown.py
--- version_11/meresco/components/facetindex/drilldown.py	2010-05-06 22:18:15.000000000 +0200
+++ version_12/meresco/components/facetindex/drilldown.py	2010-05-07 00:16:59.000000000 +0200
@@ -9,6 +9,7 @@
 #    Copyright (C) 2009-2010 Delft University of Technology http://www.tudelft.nl
 #    Copyright (C) 2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2007-2010 Seek You Too (CQ2) http://www.cq2.nl
+#    Copyright (C) 2010 Stichting Kennisnet http://www.kennisnet.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -28,7 +29,7 @@
 #
 ## end license ##
 from .docsetlist import DocSetList, JACCARD_MI
-from merescolucene import Term, IndexReader, iterJ  # hmm, maybe we don't want this dependency?
+from merescolucene import Term, IndexReader, iterJ  
 from time import time
 from sys import maxint
 from functioncommand import FunctionCommand
@@ -38,49 +39,48 @@
 
 IndexReader_FieldOption_ALL = IndexReader.FieldOption.ALL
 
-from lucene import tokenize
 
 class NoFacetIndexException(Exception):
     def __init__(self, field, fields):
         Exception.__init__(self, "No facetindex for field '%s'. Available fields: %s" % (field, ', '.join("'%s'" % field for field in fields)))
 
 class Drilldown(object):
-    def __init__(self, staticDrilldownFieldnames=None, transactionName=None, tokenize=None):
-        self._staticDrilldownFieldnames = staticDrilldownFieldnames
-        self._actualDrilldownFieldnames = self._staticDrilldownFieldnames
-        self._docsetlists = {}
-        if self._staticDrilldownFieldnames:
-            self._docsetlists = dict((fieldname, DocSetList()) for fieldname in self._staticDrilldownFieldnames)
+
+    def __init__(self, drilldownFields=None, transactionName=None):
+        drilldownFields = drilldownFields or ['*']
+        self._drilldownFields = []
+        self._prefixes = []
+        self._compoundFields = []
+        for field in drilldownFields:
+            if type(field) == tuple:
+                self._compoundFields.append(field)
+            elif field.endswith('*'):
+                self._prefixes.append(field.rstrip('*'))
+            else:
+                self._drilldownFields.append(field)
+        self._docsetlists = defaultdict(DocSetList)
         self._commandQueue = []
         self._transactionName = transactionName
-        self._tokenize = tokenize if tokenize else []
 
     def _add(self, docId, docDict):
-        keys = docDict.keys()
-        fieldnames = (fieldname
-            for fieldname in keys
-                if fieldname in self._actualDrilldownFieldnames)
-        for fieldname in fieldnames:
-            values = docDict[fieldname]
-            if fieldname in self._tokenize:
-                values = tokenize(docDict[fieldname])
-            self._docsetlists[fieldname].addDocument(docId, values)
-
-        compoundFields = (field for field in self._actualDrilldownFieldnames if type(field) == tuple)
-        values = defaultdict(set)
-        for field in compoundFields:
-            for fieldname in field:
-                if not fieldname in keys:
-                    continue
-                
-                for value in docDict[fieldname]:
-                    if field in self._tokenize:
-                        for token in tokenize(value):
-                            values[field].add(token)
-                    else:
-                        values[field].add(value)
-        for field, value in values.items():
-            self._docsetlists[field].addDocument(docId, value)
+        for field in docDict.keys():
+            if self._isDrilldownField(field):
+                self._docsetlists[field].addDocument(docId, docDict[field])
+
+        for compoundField in self._compoundFields:
+            terms = set(term for field in compoundField \
+                            for term in docDict.get(field, []))
+            self._docsetlists[compoundField].addDocument(docId, terms)
+
+    def _isDrilldownField(self, field):
+        if field.startswith('__'):
+            return False
+        if field in self._drilldownFields:
+            return True
+        for prefix in self._prefixes:
+            if field.startswith(prefix):
+                return True
+        return False
 
     def addDocument(self, docId, docDict):
         self.deleteDocument(docId)
@@ -102,35 +102,34 @@
         pass
 
     def begin(self):
-        tx = callstackscope('__callstack_var_tx__') # rather derive from Observable oid and use self.tx
+        tx = callstackscope('__callstack_var_tx__')
         if tx.name == self._transactionName:
             tx.join(self)
 
     def deleteDocument(self, docId):
         self._commandQueue.append(FunctionCommand(self._delete, docId=docId))
 
-    def indexStarted(self, indexReader, docIdMapping=None):
+    def indexStarted(self, index):
         t0 = time()
+        indexReader = index.getIndexReader()
+        docIdMapping = index.getDocIdMapping()
+        self._index = index
+        for field in self._determineDrilldownFields(indexReader):
+            self._docsetlists[field] = self._docSetListFromTermEnumForField(field, indexReader, docIdMapping)
+        for compoundField in self._compoundFields:
+            for field in compoundField:
+                self._docsetlists[compoundField].merge(self._docSetListFromTermEnumForField(field, indexReader, docIdMapping))
 
-        self._totaldocs = indexReader.numDocs()
-        fieldNames = self._staticDrilldownFieldnames
-        if not fieldNames:
-            fieldNames = [fieldname
-                for fieldname in iterJ(indexReader.getFieldNames(IndexReader_FieldOption_ALL))
-                    if not fieldname.startswith('__')]
-        for fieldname in fieldNames:
-            if type(fieldname) == tuple:
-                self._docsetlists[fieldname] = DocSetList()
-                for field in fieldname:
-                    dsl = self._docSetListFromTermEnumForField(field, indexReader, docIdMapping)
-                    self._docsetlists[fieldname].merge(dsl)
-            else:
-                self._docsetlists[fieldname] = self._docSetListFromTermEnumForField(fieldname, indexReader, docIdMapping)
-
-        self._actualDrilldownFieldnames = fieldNames
         #print 'indexStarted (ms)', (time()-t0)*1000
         #print self.measure()
 
+    def _determineDrilldownFields(self, indexReader):
+        result = set(self._drilldownFields)
+        result.update(fieldname for fieldname in \
+            iterJ(indexReader.getFieldNames(IndexReader_FieldOption_ALL)) \
+                if self._isDrilldownField(fieldname))
+        return result
+
     def _docSetListFromTermEnumForField(self, field, indexReader, docIdMapping):
         return DocSetList.forField(indexReader, field, docIdMapping)
 
@@ -138,20 +137,21 @@
         return self._docsetlists[field]
 
     def drilldown(self, docset, drilldownFieldnamesAndMaximumResults=None):
+        allActualFields = self._docsetlists.keys()
         if not drilldownFieldnamesAndMaximumResults:
             drilldownFieldnamesAndMaximumResults = [(fieldname, 0, False)
-                for fieldname in self._actualDrilldownFieldnames]
+                for fieldname in allActualFields]
         for fieldname, maximumResults, sorted in drilldownFieldnamesAndMaximumResults:
-            if fieldname not in self._actualDrilldownFieldnames:
-                raise NoFacetIndexException(fieldname, self._actualDrilldownFieldnames)
+            if fieldname not in allActualFields:
+                raise NoFacetIndexException(fieldname, allActualFields)
             yield fieldname, self._docsetlists[fieldname].termCardinalities(docset, maximumResults or maxint, sorted)
 
     def jaccard(self, docset, jaccardFieldsAndRanges, algorithm=JACCARD_MI, maxTermFreqPercentage=100):
         for fieldname, minimum, maximum in jaccardFieldsAndRanges:
             if fieldname not in self._docsetlists:
-                raise NoFacetIndexException(fieldname, self._actualDrilldownFieldnames)
+                raise NoFacetIndexException(fieldname, self._docsetlists.keys())
             yield fieldname, self._docsetlists[fieldname].jaccards(docset, minimum, maximum,
-                    self._totaldocs, algorithm=algorithm, maxTermFreqPercentage=maxTermFreqPercentage)
+                    self._index.docCount(), algorithm=algorithm, maxTermFreqPercentage=maxTermFreqPercentage)
 
     def queueLength(self):
         return len(self._commandQueue)
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/meresco/components/facetindex/fields2lucenedocument.py version_12/meresco/components/facetindex/fields2lucenedocument.py
--- version_11/meresco/components/facetindex/fields2lucenedocument.py	2010-05-06 22:18:15.000000000 +0200
+++ version_12/meresco/components/facetindex/fields2lucenedocument.py	2010-05-07 00:16:59.000000000 +0200
@@ -9,6 +9,7 @@
 #    Copyright (C) 2009 Delft University of Technology http://www.tudelft.nl
 #    Copyright (C) 2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2007-2010 Seek You Too (CQ2) http://www.cq2.nl
+#    Copyright (C) 2010 Stichting Kennisnet http://www.kennisnet.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -34,20 +35,30 @@
     def __init__(self, resourceManager, untokenized):
         self.resourceManager = resourceManager
         self.fields = {}
-        self._untokenized = untokenized
+        self._untokenized = [f for f in untokenized if not f.endswith('*')]
+        self._untokenizedPrefixes = [f.rstrip('*') for f in untokenized if f.endswith('*')]
 
     def addField(self, name, value):
         if not name in self.fields:
             self.fields[name] = []
         self.fields[name].append(value)
 
+
+    def _shouldTokenize(self, name):
+        if name in self._untokenized:
+            return False
+        for prefix in self._untokenizedPrefixes:
+            if name.startswith(prefix):
+                return False
+        return True
+
     def commit(self):
         if not self.fields.keys():
             return
         document = Document(self.resourceManager.ctx.tx.locals['id'])
         for name, values in self.fields.items():
             for value in values:
-                document.addIndexedField(name, value, not name in self._untokenized)
+                document.addIndexedField(name, value, self._shouldTokenize(name))
         self.resourceManager.do.addDocument(document)
 
     def rollback(self):
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/meresco/components/facetindex/lucene.py version_12/meresco/components/facetindex/lucene.py
--- version_11/meresco/components/facetindex/lucene.py	2010-05-06 22:18:15.000000000 +0200
+++ version_12/meresco/components/facetindex/lucene.py	2010-05-07 00:16:59.000000000 +0200
@@ -9,6 +9,7 @@
 #    Copyright (C) 2009-2010 Delft University of Technology http://www.tudelft.nl
 #    Copyright (C) 2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2007-2010 Seek You Too (CQ2) http://www.cq2.nl
+#    Copyright (C) 2010 Stichting Kennisnet http://www.kennisnet.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -36,7 +37,6 @@
 
 from time import time
 from itertools import ifilter, islice
-from StringIO import StringIO
 
 from document import IDFIELD
 from meresco.core import Observable
@@ -47,22 +47,9 @@
 
 IndexReader_FieldOption_ALL = IndexReader.FieldOption.ALL
 
-from java.io import StringReader
-from java.io import Reader
-
 class LuceneException(Exception):
     pass
 
-def tokenize(aString):
-    ts = merescoStandardAnalyzer.tokenStream('ignored fieldname', StringReader(unicode(aString)) % Reader)
-    tokens = []
-    
-    token = ts.next()
-    while token != None:
-        tokens.append(token.termText())
-        token = ts.next()
-    return tokens
-
 class _Logger(object):
     def comment(self, *strings):
         self.writeLine('# ', *strings)
@@ -135,7 +122,7 @@
         self._existingFieldNames = self._reader.getFieldNames(IndexReader_FieldOption_ALL)
 
     def observer_init(self):
-        self.do.indexStarted(self._reader, docIdMapping=self._lucene2docId)
+        self.do.indexStarted(self)
 
     def docsetFromQuery(self, pyLuceneQuery, **kwargs):
         return DocSet.fromQuery(self._searcher, pyLuceneQuery, self._lucene2docId)
@@ -238,10 +225,6 @@
     def __del__(self):
         self.close()
 
-    def start(self):
-        self._reopenIndex()
-        self.do.indexStarted(self._reader)
-
     def isOptimized(self):
         return self.docCount() == 0 or self._reader.isOptimized()
 
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/_alltests.py version_12/test/_alltests.py
--- version_11/test/_alltests.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/_alltests.py	2010-05-07 00:16:59.000000000 +0200
@@ -108,6 +108,7 @@
 from facetindex.docsettest import DocSetTest
 from facetindex.documenttest import DocumentTest
 from facetindex.drilldowntest import DrilldownTest
+from facetindex.drilldownfieldnamestest import DrilldownFieldnamesTest
 from facetindex.fields2lucenedocumenttest import Fields2LuceneDocumentTest
 from facetindex.incrementalindexingtest import IncrementalIndexingTest
 from facetindex.integerlisttest import IntegerListTest
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/alltests.sh version_12/test/alltests.sh
--- version_11/test/alltests.sh	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/alltests.sh	2010-05-07 00:16:59.000000000 +0200
@@ -22,6 +22,6 @@
 #    Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
 #
 ## end license ##
-
+export LANG=en_US.UTF-8
 export PYTHONPATH=.:"$PYTHONPATH"
-python2.5 _alltests.py $@
+python2.5 _alltests.py "$@"
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/facetindex/docsetlisttest.py version_12/test/facetindex/docsetlisttest.py
--- version_11/test/facetindex/docsetlisttest.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/facetindex/docsetlisttest.py	2010-05-07 00:16:59.000000000 +0200
@@ -224,7 +224,6 @@
         self.createBigIndex(4, 5)
         self.assertEquals(0, len(DocSetList.forField(self.reader, 'field')))
 
-    #def testAppendToRow(self):
     def testAppendDocument(self):
         docsetlist = DocSetList()
         self.assertEquals(0, len(docsetlist))
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/facetindex/documenttest.py version_12/test/facetindex/documenttest.py
--- version_11/test/facetindex/documenttest.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/facetindex/documenttest.py	2010-05-07 00:16:59.000000000 +0200
@@ -30,7 +30,7 @@
 
 import unittest
 from cq2utils import CallTrace
-from meresco.components.facetindex.document import IDFIELD, Document, DocumentException
+from meresco.components.facetindex.document import IDFIELD, Document, DocumentException, tokenize
 
 class DocumentTest(unittest.TestCase):
 
@@ -101,15 +101,60 @@
         d.addIndexedField('field', 'value1')
         d.addIndexedField('field', 'value2')
         self.assertEquals({'__id__': ['1234'], 'field': ['value1', 'value2']}, d.asDict())
-        
-    def testTokenizedFields(self):
-        d = Document('1234')
-        self.assertEquals([], d.tokenizedFields())
-        
-        d.addIndexedField('field1', 'value1')
-        d.addIndexedField('field2', 'value2.1')
-        self.assertEquals(['field1', 'field2'], d.tokenizedFields())
-        
-        d.addIndexedField('field2', 'value2.2')
-        self.assertEquals(['field1', 'field2'], d.tokenizedFields())
+
+    def testTokenize(self):
+        self.assertEquals([], tokenize(''))
+        self.assertEquals(['token'], tokenize('token'))
+        self.assertEquals(['token'], tokenize('TOKEN'))
+        self.assertEquals(['token'], tokenize('token.'))
+        self.assertEquals(['token'], tokenize("token's"))
+        self.assertEquals(['token'], tokenize('t.o.k.e.n.'))
+        self.assertEquals(['this', 'is', 'a', 'text'], tokenize('This is a text.'))
+
+    def testDictWithTokenizedFields(self):
+        d = Document('id')
+        d.addIndexedField('tokenized', 'value1 value2', tokenize=True)
+        d.addIndexedField('untokenized', 'value1 value2', tokenize=False)
+        self.assertEquals({
+            '__id__': ['id'],
+            'tokenized': ['value1', 'value2'],
+            'untokenized': ['value1 value2']
+            }, d.asDict())
+
+    def testDictWithSometimesTokenizedFields(self):
+        d = Document('id')
+        d.addIndexedField('fieldname', 'value1 value2', tokenize=True)
+        d.addIndexedField('fieldname', 'value1 value2', tokenize=False)
+        self.assertEquals({
+            '__id__': ['id'],
+            'fieldname': ['value1', 'value2', 'value1 value2']
+            }, d.asDict())
+
+
+    def testDictHasLazyTokenization(self):
+        tokenizer = CallTrace('tokenizer', methods={'tokenize': lambda x:x.split()})
+        d = Document('id')
+        d._tokenize = tokenizer.tokenize
+        d.addIndexedField('field1', 'value1 value2', tokenize=True)
+        d.addIndexedField('field2', 'value3 value4', tokenize=True)
+        docDict = d.asDict()
+        self.assertEquals(['value1', 'value2'], docDict['field1'])
+        self.assertEquals(1, len(tokenizer.calledMethods))
+
+    def testDictKeys(self):
+        d = Document('id')
+        d.addIndexedField('field1', 'value1 value2', tokenize=True)
+        d.addIndexedField('field1', 'value3 value4', tokenize=True)
+        d.addIndexedField('field2', 'value5 value6', tokenize=True)
+        self.assertEquals(set([IDFIELD, 'field1', 'field2']), d.asDict().keys())
+
+    def testDictGet(self):
+        d = Document('id')
+        d.addIndexedField('field1', 'value1 value2', tokenize=True)
+        d.addIndexedField('field2', 'value3 value4', tokenize=True)
+        self.assertEquals(['value1', 'value2'], d.asDict().get('field1'))
+        self.assertEquals(['nothing'], d.asDict().get('doesnotexist', ['nothing']))
+
+#def testDictUniqueKeysElements(self):
+
         
\ No newline at end of file
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/facetindex/drilldownfieldnamestest.py version_12/test/facetindex/drilldownfieldnamestest.py
--- version_11/test/facetindex/drilldownfieldnamestest.py	1970-01-01 01:00:00.000000000 +0100
+++ version_12/test/facetindex/drilldownfieldnamestest.py	2010-05-07 00:16:59.000000000 +0200
@@ -0,0 +1,45 @@
+## begin license ##
+#
+#    Meresco Components are components to build searchengines, repositories
+#    and archives, based on Meresco Core.
+#    Copyright (C) 2010 Seek You Too (CQ2) http://www.cq2.nl
+#
+#    This file is part of Meresco Components.
+#
+#    Meresco Components is free software; you can redistribute it and/or modify
+#    it under the terms of the GNU General Public License as published by
+#    the Free Software Foundation; either version 2 of the License, or
+#    (at your option) any later version.
+#
+#    Meresco Components is distributed in the hope that it will be useful,
+#    but WITHOUT ANY WARRANTY; without even the implied warranty of
+#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#    GNU General Public License for more details.
+#
+#    You should have received a copy of the GNU General Public License
+#    along with Meresco Components; if not, write to the Free Software
+#    Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+#
+## end license ##
+
+from cq2utils import CQ2TestCase, CallTrace
+from meresco.components.facetindex.drilldownfieldnames import DrilldownFieldnames
+
+class DrilldownFieldnamesTest(CQ2TestCase):
+
+    def testDrilldownFieldnames(self):
+        d = DrilldownFieldnames(
+            lookup=lambda name: 'drilldown.'+name,
+            reverse=lambda name: name[len('drilldown.'):])
+        observer = CallTrace('drilldown')
+        observer.returnValues['drilldown'] = [('drilldown.field1', [('term1',1)]),('drilldown.field2', [('term2', 2)])]
+        d.addObserver(observer)
+        hits = CallTrace('Hits')
+
+        result = list(d.drilldown(hits, [('field1', 0, True),('field2', 3, False)]))
+
+        self.assertEquals(1, len(observer.calledMethods))
+        self.assertEquals([('drilldown.field1', 0, True),('drilldown.field2', 3, False)], list(observer.calledMethods[0].args[1]))
+
+        self.assertEquals([('field1', [('term1',1)]),('field2', [('term2', 2)])], result)
+
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/facetindex/drilldowntest.py version_12/test/facetindex/drilldowntest.py
--- version_11/test/facetindex/drilldowntest.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/facetindex/drilldowntest.py	2010-05-07 00:16:59.000000000 +0200
@@ -9,6 +9,7 @@
 #    Copyright (C) 2009-2010 Delft University of Technology http://www.tudelft.nl
 #    Copyright (C) 2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2007-2010 Seek You Too (CQ2) http://www.cq2.nl
+#    Copyright (C) 2010 Stichting Kennisnet http://www.kennisnet.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -34,7 +35,6 @@
 
 from meresco.components.facetindex.document import Document
 from meresco.components.facetindex.drilldown import Drilldown, NoFacetIndexException
-from meresco.components.facetindex.drilldownfieldnames import DrilldownFieldnames
 from meresco.components.facetindex.lucene import LuceneIndex
 
 from meresco.components.facetindex.docset import DocSet
@@ -52,6 +52,11 @@
         self.index.close()
         CQ2TestCase.tearDown(self)
 
+    def createDrilldown(self, *args, **kwargs):
+        self.drilldown = Drilldown(*args, **kwargs)
+        self.index.addObserver(self.drilldown)
+        self.index.observer_init()
+
     #Helper functions:
     def addUntokenized(self, documents, index=None):
         self.addToIndex(documents, index=index)
@@ -69,35 +74,30 @@
                 myDocument.addIndexedField(field, value, tokenize = tokenize)
             index.addDocument(myDocument)
         index.commit()
+        if hasattr(self, 'drilldown'):
+            self.drilldown.commit()
 
     def testIndexStarted(self):
+        self.createDrilldown(['field_0'])
         self.addUntokenized([('id', {'field_0': 'this is term_0'})])
 
-        drilldown = Drilldown(['field_0'])
-        reader = IndexReader.open(self.tempdir)
-
-        drilldown.indexStarted(reader, docIdMapping=self.index.getDocIdMapping())
-        field, results = drilldown.drilldown(DocSet(data=[0]), [('field_0', 10, False)]).next()
+        field, results = self.drilldown.drilldown(DocSet(data=[0]), [('field_0', 10, False)]).next()
         self.assertEquals('field_0', field)
         self.assertEquals([('this is term_0', 1)], list(results))
 
     def testDrilldown(self):
+        self.createDrilldown(['field_0', 'field_1'])
         self.addUntokenized([
             ('0', {'field_0': 'this is term_0', 'field_1': 'inquery'}),
             ('1', {'field_0': 'this is term_1', 'field_1': 'inquery'}),
             ('2', {'field_0': 'this is term_1', 'field_1': 'inquery'}),
             ('3', {'field_0': 'this is term_2', 'field_1': 'cannotbefound'})])
-        self.index._writer.flush()
-        reader = IndexReader.open(self.tempdir)
-        #convertor = LuceneRawDocSets(reader, ['field_0', 'field_1'])
-        drilldown = Drilldown(['field_0', 'field_1'])
-        drilldown.indexStarted(reader, docIdMapping=self.index.getDocIdMapping())
         query = TermQuery(Term("field_1", "inquery"))
         total, queryResults = self.index.executeQuery(query)
         self.assertEquals(3, total)
         self.assertEquals(['0', '1', '2'], queryResults)
         queryDocset = self.index.docsetFromQuery(query)
-        drilldownResult = list(drilldown.drilldown(queryDocset, [('field_0', 0, False), ('field_1', 0, False)]))
+        drilldownResult = list(self.drilldown.drilldown(queryDocset, [('field_0', 0, False), ('field_1', 0, False)]))
         self.assertEquals(2, len(drilldownResult))
         result = dict(drilldownResult)
         self.assertEquals(['field_0', 'field_1'], result.keys())
@@ -105,113 +105,73 @@
         self.assertEquals([("inquery", 3)], list(result['field_1']))
 
     def testSortingOnCardinality(self):
+        self.createDrilldown(['field0'])
         self.addUntokenized([
             ('0', {'field0': 'term1'}),
             ('1', {'field0': 'term1'}),
             ('2', {'field0': 'term2'}),
             ('3', {'field0': 'term0'})])
-        reader = IndexReader.open(self.tempdir)
-        drilldown = Drilldown(['field0'])
-        drilldown.indexStarted(reader, docIdMapping=self.index.getDocIdMapping())
         hits = self.index.docsetFromQuery(MatchAllDocsQuery())
-        ddData = list(drilldown.drilldown(hits, [('field0', 0, False)]))
+        ddData = list(self.drilldown.drilldown(hits, [('field0', 0, False)]))
         self.assertEquals([('term0',1), ('term1',2), ('term2',1)], list(ddData[0][1]))
-        result = list(drilldown.drilldown(hits, [('field0', 0, True)]))
+        result = list(self.drilldown.drilldown(hits, [('field0', 0, True)]))
         self.assertEquals([('term1',2), ('term0',1), ('term2',1)], list(result[0][1]))
 
-
-    def testAppendToRow(self):
-        docsetlist = DocSetList()
-        docsetlist.addDocument(0, ['term0', 'term1'])
-        self.assertEquals(set(['term0', 'term1']), set(docsetlist.termForDocset(docsetlist[i]) for i in range(2)))
-        self.assertEquals([('term0', 1), ('term1', 1)], list(docsetlist.termCardinalities(DocSet([0, 1]))))
-        docsetlist.addDocument(1, ['term0', 'term1'])
-        self.assertEquals('term0', docsetlist.termForDocset(docsetlist[0]))
-        self.assertEquals('term1', docsetlist.termForDocset(docsetlist[1]))
-        self.assertEquals([('term0', 2), ('term1', 2)], list(docsetlist.termCardinalities(DocSet([0, 1]))))
-        docsetlist.addDocument(2, ['term0', 'term2'])
-        self.assertEquals([('term0', 3), ('term1', 2), ('term2', 1)], list(docsetlist.termCardinalities(DocSet([0, 1, 2]))))
-        try:
-            docsetlist.addDocument(2, ['term0', 'term2'])
-        except Exception, e:
-            self.assertTrue("non-increasing" in str(e))
-
     def testDynamicDrilldownFields(self):
+        self.createDrilldown(['*'])
         self.addUntokenized([
             ('0', {'field_0': 'this is term_0', 'field_1': 'inquery'}),
             ('1', {'field_0': 'this is term_1', 'field_1': 'inquery'}),
             ('2', {'field_0': 'this is term_1', 'field_1': 'inquery'}),
             ('3', {'__private_field': 'this is term_2', 'field_1': 'cannotbefound'})])
-        reader = IndexReader.open(self.tempdir)
-        drilldown = Drilldown()
-        drilldown.indexStarted(reader, docIdMapping=self.index.getDocIdMapping())
         docset = self.index.docsetFromQuery(MatchAllDocsQuery())
-        results = list(drilldown.drilldown(docset, [('field_0', 0, False)]))
+        results = list(self.drilldown.drilldown(docset, [('field_0', 0, False)]))
         self.assertEquals('field_0', results[0][0])
-        results = list(drilldown.drilldown(docset))
+        results = list(self.drilldown.drilldown(docset))
         self.assertEquals('field_0', results[0][0])
         self.assertEquals('field_1', results[1][0])
         self.assertEquals(2, len(results))
 
     def testFieldGetAdded(self):
+        self.createDrilldown(['*'])
         self.addUntokenized([
             ('0', {'field_0': 'this is term_0'})
         ])
-        drilldown = Drilldown()
-        drilldown.indexStarted(self.index.getIndexReader(), docIdMapping=self.index.getDocIdMapping())
         docset = self.index.docsetFromQuery(MatchAllDocsQuery())
-        results = list(drilldown.drilldown(docset))
+        results = list(self.drilldown.drilldown(docset))
         self.assertEquals('field_0', results[0][0])
         self.assertEquals(1, len(results))
         self.addUntokenized([
             ('1', {'field_0': 'this is term_0', 'field_1': 'inquery'})
         ])
-        drilldown.indexStarted(self.index.getIndexReader(), docIdMapping=self.index.getDocIdMapping())
         docset = self.index.docsetFromQuery(MatchAllDocsQuery())
-        results = list(drilldown.drilldown(docset))
+        results = list(self.drilldown.drilldown(docset))
         self.assertEquals(2, len(results))
         self.assertEquals('field_0', results[0][0])
         self.assertEquals('field_1', results[1][0])
 
-    def testDrilldownFieldnames(self):
-        d = DrilldownFieldnames(
-            lookup=lambda name: 'drilldown.'+name,
-            reverse=lambda name: name[len('drilldown.'):])
-        observer = CallTrace('drilldown')
-        observer.returnValues['drilldown'] = [('drilldown.field1', [('term1',1)]),('drilldown.field2', [('term2', 2)])]
-        d.addObserver(observer)
-        hits = CallTrace('Hits')
-
-        result = list(d.drilldown(hits, [('field1', 0, True),('field2', 3, False)]))
-
-        self.assertEquals(1, len(observer.calledMethods))
-        self.assertEquals([('drilldown.field1', 0, True),('drilldown.field2', 3, False)], list(observer.calledMethods[0].args[1]))
-
-        self.assertEquals([('field1', [('term1',1)]),('field2', [('term2', 2)])], result)
-
     def testJaccardIndex(self):
+        self.createDrilldown(['title'])
         self.addTokenized([
             ('0', {'title': 'cats dogs mice'}),
             ('1', {'title': 'cats dogs'}),
             ('2', {'title': 'cats'}),
             ('3', {'title': 'dogs mice'})])
-        self.index._writer.flush()
-        reader = IndexReader.open(self.tempdir)
+        # The following line fixes test, but in the wrong way
+        #self.drilldown.indexStarted(self.index)
 
-        drilldown = Drilldown(['title'])
-        drilldown.indexStarted(reader, docIdMapping=self.index.getDocIdMapping())
         query = TermQuery(Term("title", "dogs"))
         total, queryResults = self.index.executeQuery(query)
         queryDocset = self.index.docsetFromQuery(query)
-        jaccardIndices = list(drilldown.jaccard(queryDocset, [("title", 0, 100)], algorithm=JACCARD_ONLY))
+        jaccardIndices = list(self.drilldown.jaccard(queryDocset, [("title", 0, 100)], algorithm=JACCARD_ONLY))
         self.assertEquals([('title', [('dogs',100),('mice', 66),('cats',50)])], list((fieldname, list(items)) for fieldname, items in jaccardIndices))
 
-        jaccardIndices = list(drilldown.jaccard(queryDocset, [("title", 45, 55)], algorithm=JACCARD_ONLY))
+        jaccardIndices = list(self.drilldown.jaccard(queryDocset, [("title", 45, 55)], algorithm=JACCARD_ONLY))
         self.assertEquals([('title', [('cats',50)])], list((fieldname, list(items)) for fieldname, items in jaccardIndices))
 
     def testJaccardPassMaxTermFreqPercentage(self):
         drilldown = Drilldown([])
-        drilldown._totaldocs = 78
+        drilldown._index = CallTrace('index', returnValues={'docCount':78})
         docSetList_for_title = CallTrace('DocSetList')
         drilldown._docsetlists['title'] = docSetList_for_title
         list(drilldown.jaccard(None, [("title", 17, 67)], maxTermFreqPercentage=80))
@@ -219,9 +179,9 @@
         self.assertEquals("[jaccards(None, 17, 67, 78, algorithm=%s, maxTermFreqPercentage=80)]" % algorithm, str(docSetList_for_title.calledMethods))
 
     def testJaccardIndexChecksFields(self):
-        drilldown = Drilldown(['title'])
+        self.createDrilldown(['title'])
         try:
-            jaccardIndex = list(drilldown.jaccard(None, [("name", 0, 100)]))
+            jaccardIndex = list(self.drilldown.jaccard(None, [("name", 0, 100)]))
             self.fail()
         except NoFacetIndexException, e:
             self.assertEquals("No facetindex for field 'name'. Available fields: 'title'", str(e))
@@ -258,9 +218,7 @@
         self.assertEquals(0, drilldown.queueLength())
 
     def testCommit(self):
-        reader = IndexReader.open(self.tempdir)
         drilldown = Drilldown(['title'])
-        drilldown.indexStarted(reader, docIdMapping=self.index.getDocIdMapping())
         drilldown.addDocument(0, {'title': ['value']})
         drilldown.addDocument(1, {'title': ['value2']})
 
@@ -277,21 +235,18 @@
         self.assertEquals([('value', 0), ('value2', 1)], list(drilldown._docsetlists['title'].allCardinalities()))
 
     def testMapLuceneIdsOnStartup(self):
-        drilldown = Drilldown(['field_0'])
-        self.index.addObserver(drilldown)
         documents = []
         for i in range(8):
             recordId = 'id:%0.3d' % i
             data = {'field_0': 'value%0.3d' % i}
             documents.append((recordId, data))
         self.addUntokenized(documents)
-        drilldown.commit()
         self.index.close()
 
         #print "---- 1 ----"
         index = LuceneIndex(self.tempdir)
         drilldown = Drilldown(['field_0'])
-        drilldown.indexStarted(index._reader, docIdMapping=index.getDocIdMapping())
+        drilldown.indexStarted(index)
 
         index.delete('id:003')
         index.delete('id:006')
@@ -302,7 +257,7 @@
         #print "---- 2 ----"
         index2 = LuceneIndex(self.tempdir)
         drilldown2 = Drilldown(['field_0'])
-        drilldown2.indexStarted(index2._reader, docIdMapping=index2.getDocIdMapping())
+        drilldown2.indexStarted(index2)
         documents = []
         for i in range(8,89):
             recordId = 'id:%0.3d' % i
@@ -316,7 +271,7 @@
         #print "---- 3 ----"
         index3 = LuceneIndex(self.tempdir)
         drilldown3 = Drilldown(['field_0'])
-        drilldown3.indexStarted(index3._reader, docIdMapping=index3.getDocIdMapping())
+        drilldown3.indexStarted(index3)
         drilldownDocIds = [x[0] for x in list(drilldown3._docsetlists['field_0'])]
 
         self.assertEquals([0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], drilldownDocIds)
@@ -378,30 +333,49 @@
         self.assertEquals(set([('math', 1), ('mathematics for dummies', 1)]), set(resultTerms))
 
     def testIndexStartedWithCompoundField(self):
+        self.createDrilldown(['field_0', ('field_0', 'field_1')])
         self.addUntokenized([('id0', {'field_0': 'this is term_0'})])
         self.addUntokenized([('id1', {'field_1': 'this is term_1'})])
 
-        drilldown = Drilldown(['field_0', ('field_0', 'field_1')])
-        reader = IndexReader.open(self.tempdir)
-
-        drilldown.indexStarted(reader, docIdMapping=self.index.getDocIdMapping())
-        field, results = drilldown.drilldown(DocSet(data=[0,1]), [(('field_0', 'field_1'), 10, False)]).next()
+        field, results = self.drilldown.drilldown(DocSet(data=[0,1]), [(('field_0', 'field_1'), 10, False)]).next()
         self.assertEquals(('field_0', 'field_1'), field)
         self.assertEquals([('this is term_0', 1), ('this is term_1', 1)], list(results))
 
+    def testDetermineDrilldownFieldnamesWithoutStars(self):
+        self.addUntokenized([('id0', {
+            'prefix.field_0': 'this is term_0',
+            'prefix.field_1': 'this is term_1',
+            'field_2': 'this is term_2'})])
+        drilldown = Drilldown(['field_2'])
+        fields = drilldown._determineDrilldownFields(self.index.getIndexReader())
+        self.assertEquals(set(['field_2']), fields)
+
+    def testDetermineDrilldownFieldnamesWithAllstars(self):
+        self.addUntokenized([('id0', {
+            'prefix.field_0': 'this is term_0',
+            'prefix.field_1': 'this is term_1',
+            'field_2': 'this is term_2'})])
+        drilldown = Drilldown(['*'])
+        fields = drilldown._determineDrilldownFields(self.index.getIndexReader())
+        self.assertEquals(set(['field_2', 'prefix.field_0', 'prefix.field_1']) , fields)
+
+    def testDetermineDrilldownFieldnamesWithPrefixStar(self):
+        self.addUntokenized([('id0', {
+            'prefix.field_0': 'this is term_0',
+            'prefix.field_1': 'this is term_1',
+            'field_2': 'this is term_2'})])
+        drilldown = Drilldown(['prefix.*'])
+        fields = drilldown._determineDrilldownFields(self.index.getIndexReader())
+        self.assertEquals(set(['prefix.field_0', 'prefix.field_1']) , fields)
+    def testDrilldownFieldnamesWithPrefixStar(self):
+        self.createDrilldown(['prefix.*'])
+        self.addUntokenized([('id0', {
+            'prefix.field_0': 'this is term_0',
+            'prefix.field_1': 'this is term_1',
+            'field_2': 'this is term_2'})])
+        field, results = self.drilldown.drilldown(DocSet(data=[0]), [('prefix.field_0', 10, False)]).next()
+        self.assertEquals([('this is term_0', 1)], list(results))
+
     def testCompoundFieldWithSameTermInDifferentFields(self):
         drilldown = Drilldown([('field_0', 'field_1')])
         drilldown._add(0, {'field_0': ['value'], 'field_1': ['value']}) # had a bug causing: "non-increasing docid" error
-
-    def testTokenize(self):
-        drilldown = Drilldown(['tokenized', 'untokenized', ('tokenized', 'untokenized')], tokenize=['tokenized', ('tokenized', 'untokenized')])
-        drilldown.addDocument(0, {'tokenized': ['token one'], 'untokenized': ['token two']})
-        drilldown.addDocument(1, {'tokenized': ['token two'], 'untokenized': ['token two']})
-        drilldown.commit()
-        self.assertEquals([('token', 2), ('one', 1), ('two', 1)], list(drilldown._docsetlists['tokenized'].allCardinalities()))
-        self.assertEquals(
-            [('token two', 2)], 
-            list(drilldown._docsetlists['untokenized'].allCardinalities()))
-        self.assertEquals(
-            set([('token', 2), ('one', 1), ('two', 2)]), 
-            set(drilldown._docsetlists[('tokenized','untokenized')].allCardinalities()))
\ No newline at end of file
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/facetindex/fields2lucenedocumenttest.py version_12/test/facetindex/fields2lucenedocumenttest.py
--- version_11/test/facetindex/fields2lucenedocumenttest.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/facetindex/fields2lucenedocumenttest.py	2010-05-07 00:16:59.000000000 +0200
@@ -9,6 +9,7 @@
 #    Copyright (C) 2009 Delft University of Technology http://www.tudelft.nl
 #    Copyright (C) 2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2007-2010 Seek You Too (CQ2) http://www.cq2.nl
+#    Copyright (C) 2010 Stichting Kennisnet http://www.kennisnet.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -30,7 +31,7 @@
 from unittest import TestCase
 from cq2utils import CallTrace
 from meresco.core import be, Transparant, Observable
-from meresco.core import TransactionScope, ResourceManager
+from meresco.core import TransactionScope, ResourceManager, Transaction
 
 from meresco.components.facetindex import Fields2LuceneDocumentTx, Document
 from meresco.components.facetindex.merescolucene import iterJ
@@ -83,13 +84,19 @@
         self.assertEquals([Document], [type(arg) for arg in self.observert.calledMethods[1].args])
 
         document = self.observert.calledMethods[1].args[0]
-        self.assertEquals([u'TermOne', u'TermTwo'], list(iterJ(document._document.getValues('a'))))
+        indexwriter = CallTrace('IndexWriter')
+        document.addToIndexWith(indexwriter)
+        luceneDocument = indexwriter.calledMethods[0].args[0]
+        self.assertEquals([u'TermOne', u'TermTwo'], list(iterJ(luceneDocument.getValues('a'))))
 
     def testTokenizedIsNotForgotten(self):
         list(self.body.all.addFields([('a', '1'), ('a', 'termone termtwo'), ('b', 'termone termtwo')]))
         document = self.observert.calledMethods[1].args[0]
-        self.assertTrue(document._document.getField('a').isTokenized())
-        self.assertFalse(document._document.getField('b').isTokenized())
+        indexwriter = CallTrace('IndexWriter')
+        document.addToIndexWith(indexwriter)
+        luceneDocument = indexwriter.calledMethods[0].args[0]
+        self.assertTrue(luceneDocument.getField('a').isTokenized())
+        self.assertFalse(luceneDocument.getField('b').isTokenized())
 
     def testRollback(self):
         fields = Fields2LuceneDocumentTx(CallTrace('Transaction'), [])
@@ -100,3 +107,23 @@
         self.body.do.addFields([('a', 'TermOne'), ('a', 'TermTwo'), ('b', '3')], 'theIdentifier')
         doc = self.observert.calledMethods[1].args[0]
         self.assertEquals('theIdentifier', doc.identifier)
+
+    def testUntokenizedSupportsPrefixes(self):
+        __callstack_var_tx__ = Transaction('transaction')
+        __callstack_var_tx__.locals['id'] = 'id'
+        resourceMgr = ResourceManager('transaction', 'ignored')
+        observer = CallTrace('observer')
+        resourceMgr.addObserver(observer)
+        f2doc = Fields2LuceneDocumentTx(resourceMgr, untokenized=['prefix.*', 'fieldname'])
+        f2doc.addField('fieldname', 'value1 value2')
+        f2doc.addField('tokenized', 'value1 value2')
+        f2doc.addField('prefix.field', 'value1 value2')
+        f2doc.commit()
+
+        document = observer.calledMethods[0].args[0]
+        d = document.asDict()
+        self.assertEquals(['value1 value2'], d['fieldname'])
+        self.assertEquals(['value1', 'value2'], d['tokenized'])
+        self.assertEquals(['value1 value2'], d['prefix.field'])
+
+
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/facetindex/incrementalindexingtest.py version_12/test/facetindex/incrementalindexingtest.py
--- version_11/test/facetindex/incrementalindexingtest.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/facetindex/incrementalindexingtest.py	2010-05-07 00:16:59.000000000 +0200
@@ -9,6 +9,7 @@
 #    Copyright (C) 2009 Delft University of Technology http://www.tudelft.nl
 #    Copyright (C) 2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2007-2010 Seek You Too (CQ2) http://www.cq2.nl
+#    Copyright (C) 2010 Stichting Kennisnet http://www.kennisnet.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -39,7 +40,7 @@
         self.index = LuceneIndex(self.tempdir)
         self.drilldown = Drilldown(['field0'])
         self.index.addObserver(self.drilldown)
-        self.index.start()
+        self.index.observer_init()
 
     def addDocument(self, identifier, **fields):
         doc = Document(identifier)
@@ -99,10 +100,9 @@
         self.assertEquals(1, self.index.queueLength())     # add
         self.assertEquals(['_add'], [command.methodName() for command in self.index._commandQueue])
         self.assertEquals(2, self.drilldown.queueLength()) # delete, add
-        self.assertEquals([
-            '_delete(docId=0)',
-            "_add(docDict={u'field0': [u'term0'], u'__id__': [u'1']}, docId=0)",
-            ], list(repr(command) for command in self.drilldown._commandQueue))
+        self.assertEquals(['_delete', '_add'], [command.methodName() for command in self.drilldown._commandQueue])
+        self.assertEquals([0, 0], [command._kwargs['docId'] for command in self.drilldown._commandQueue])
+        self.assertEquals({'field0': [u'term0'], u'__id__': [u'1']}, self.drilldown._commandQueue[1]._kwargs['docDict'])
 
     def testAddDocumentAndThenDeleteIt(self):
         self.assertEquals(0, self.index.queueLength())
@@ -111,11 +111,10 @@
         self.index.delete('1')                # internally: delete, delegated to drilldown
         self.assertEquals(2, self.index.queueLength())     # add, delete
         self.assertEquals(['_add', '_delete'], [command.methodName() for command in self.index._commandQueue])
-        #self.assertEquals(2, self.drilldown.queueLength()) # add, delete
-        self.assertEquals([
-            '_delete(docId=0)',
-            "_add(docDict={u'field0': [u'term0'], u'__id__': [u'1']}, docId=0)",
-            '_delete(docId=0)'], list(repr(command) for command in self.drilldown._commandQueue))
+        self.assertEquals(3, self.drilldown.queueLength()) # add, delete
+        self.assertEquals(['_delete', '_add', '_delete'], [command.methodName() for command in self.drilldown._commandQueue])
+        self.assertEquals([0, 0, 0], [command._kwargs['docId'] for command in self.drilldown._commandQueue])
+        self.assertEquals({'field0': [u'term0'], u'__id__': [u'1']}, self.drilldown._commandQueue[1]._kwargs['docDict'])
 
     def testDeleteDocumentAndThenAddIt(self):
         self.addDocument('1', field0='term0')  # add docId=0
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/facetindex/lucenetest.py version_12/test/facetindex/lucenetest.py
--- version_11/test/facetindex/lucenetest.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/facetindex/lucenetest.py	2010-05-07 00:16:59.000000000 +0200
@@ -10,6 +10,7 @@
 #    Copyright (C) 2009-2010 Delft University of Technology http://www.tudelft.nl
 #    Copyright (C) 2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2007-2010 Seek You Too (CQ2) http://www.cq2.nl
+#    Copyright (C) 2010 Stichting Kennisnet http://www.kennisnet.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -45,8 +46,6 @@
 
 from weightless import Reactor
 
-from meresco.components.facetindex.lucene import tokenize
-
 class LuceneTest(CQ2TestCase):
 
     def setUp(self):
@@ -61,17 +60,6 @@
         self.assertEquals(os.path.isdir(self.tempdir), True)
         self.assertTrue(IndexReader.indexExists(self.tempdir))
 
-    def testTokenize(self):
-        self.assertEquals([], tokenize(''))
-        self.assertEquals(['token'], tokenize('token'))
-        self.assertEquals(['token'], tokenize('TOKEN'))
-        self.assertEquals(['token'], tokenize('token.'))
-        self.assertEquals(['token'], tokenize("token's"))
-        self.assertEquals(['token'], tokenize('t.o.k.e.n.'))
-        self.assertEquals(['this', 'is', 'a', 'text'], tokenize('This is a text.'))
-        
-        
-
     def testAddToIndex(self):
         myDocument = Document('0123456789')
         myDocument.addIndexedField('title', 'een titel')
@@ -239,12 +227,11 @@
         self.assertEquals(['0123456789'], hits1)
         self.assertEquals(['0123456789'], hits2)
 
-
-    def testStart(self):
+    def testObserverInit(self):
         intercept = CallTrace('Interceptor')
         self._luceneIndex.addObserver(intercept)
 
-        self._luceneIndex.start()
+        self._luceneIndex.observer_init()
 
         self.assertEquals(1, len(intercept.calledMethods))
         self.assertEquals('indexStarted', intercept.calledMethods[0].name)
@@ -461,14 +448,16 @@
         self.addDocument('2', field1='nut')
         self.assertEquals(1, len(observer.calledMethods))
         self.assertEquals(1, len(self._luceneIndex._commandQueue))
-        self.assertEquals("addDocument(docDict={u'field1': [u'nut'], u'__id__': [u'2']}, docId=0)", str(observer.calledMethods[0]))
+        self.assertEquals("addDocument", observer.calledMethods[0].name)
+        self.assertEquals({'docDict': {u'field1': [u'nut'], u'__id__': [u'2']}, 'docId': 0}, observer.calledMethods[0].kwargs)
 
     def testPassOnDeleteDocument(self):
         observer = CallTrace('observer')
         self._luceneIndex.addObserver(observer)
         self.addDocument('identifier', field0='term0') # 0
         self._luceneIndex.delete('identifier')
-        self.assertEquals("addDocument(docDict={u'field0': [u'term0'], u'__id__': [u'identifier']}, docId=0)", str(observer.calledMethods[0]))
+        self.assertEquals("addDocument", observer.calledMethods[0].name)
+        self.assertEquals({'docDict':{u'field0': [u'term0'], u'__id__': [u'identifier']}, 'docId':0}, observer.calledMethods[0].kwargs)
         self.assertEquals('deleteDocument(docId=0)', str(observer.calledMethods[1]))
 
     def testDocIdTrackerIntegration(self):
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/ngram/ngramindextest.py version_12/test/ngram/ngramindextest.py
--- version_11/test/ngram/ngramindextest.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/ngram/ngramindextest.py	2010-05-07 00:16:59.000000000 +0200
@@ -2,12 +2,9 @@
 #
 #    Meresco Components are components to build searchengines, repositories
 #    and archives, based on Meresco Core.
-#    Copyright (C) 2008 Delft University of Technology http://www.tudelft.nl
-#    Copyright (C) 2008 Tilburg University http://www.uvt.nl
+#    Copyright (C) 2008-2009 Delft University of Technology http://www.tudelft.nl
+#    Copyright (C) 2008-2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2008-2010 Seek You Too (CQ2) http://www.cq2.nl
-#    Copyright (C) 2009 Delft University of Technology http://www.tudelft.nl
-#    Copyright (C) 2009 Tilburg University http://www.uvt.nl
-#    Copyright (C) 2009-2010 Seek You Too (CQ2) http://www.cq2.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -50,7 +47,7 @@
         self.assertEquals(['addDocument'], [m.name for m in index.calledMethods])
         doc = index.calledMethods[0].args[0]
         self.assertEquals('value$', doc.identifier)
-        self.assertEquals(['va al lu ue'], doc.asDict()['ngrams'])
+        self.assertEquals(['va', 'al', 'lu', 'ue'], doc.asDict()['ngrams'])
         
     def testIndexMultipleField(self):
         __callstack_var_tx__ = Transaction('record')
@@ -87,7 +84,7 @@
         ngramindex.commit()
 
         self.assertEquals(['addDocument', 'addDocument', 'addDocument'], [m.name for m in index.calledMethods])
-        docs = [(doc.identifier, doc.asDict()[NGRAMS_FIELD][0], doc.asDict()[NAME_FIELD][0]) for doc in (m.args[0] for m in index.calledMethods)]
+        docs = [(doc.identifier, ' '.join(doc.asDict()[NGRAMS_FIELD]), doc.asDict()[NAME_FIELD][0]) for doc in (m.args[0] for m in index.calledMethods)]
         docs.sort()
         self.assertEquals(('value$', 'va al lu ue', 'ngrams$'), docs[0])
         self.assertEquals(('value3$', 'va al lu ue e3', 'ngrams$'), docs[1])
diff --unidirectional-new-file --exclude='*.so' --exclude='*.o' --exclude=.svn --exclude='*.pyc' --exclude=deps.d --exclude=applied --recursive --unified version_11/test/ngram/ngramtest.py version_12/test/ngram/ngramtest.py
--- version_11/test/ngram/ngramtest.py	2010-05-06 22:18:14.000000000 +0200
+++ version_12/test/ngram/ngramtest.py	2010-05-07 00:16:59.000000000 +0200
@@ -3,12 +3,9 @@
 #
 #    Meresco Components are components to build searchengines, repositories
 #    and archives, based on Meresco Core.
-#    Copyright (C) 2008 Delft University of Technology http://www.tudelft.nl
-#    Copyright (C) 2008 Tilburg University http://www.uvt.nl
+#    Copyright (C) 2008-2009 Delft University of Technology http://www.tudelft.nl
+#    Copyright (C) 2008-2009 Tilburg University http://www.uvt.nl
 #    Copyright (C) 2008-2010 Seek You Too (CQ2) http://www.cq2.nl
-#    Copyright (C) 2009 Delft University of Technology http://www.tudelft.nl
-#    Copyright (C) 2009 Tilburg University http://www.uvt.nl
-#    Copyright (C) 2009-2010 Seek You Too (CQ2) http://www.cq2.nl
 #
 #    This file is part of Meresco Components.
 #
@@ -81,6 +78,7 @@
                 )
             )
         ))
+        self.indexingDna.once.observer_init()
         suggesterDna = be((Observable(),
             (NoOpSuggester(),
                 (NGramQuery(2, samples=50, fieldnames=['field0', 'field1'], fieldForSorting='allfields'),
@@ -162,8 +160,8 @@
 
         inclusive, suggestions = self.suggestionsFor('puch')
         firstletters = ''.join(word[0] for word in suggestions)
-        self.assertEquals('ccccc', firstletters[:5])
-        self.assertFalse('c' in firstletters[5:], firstletters)
+        self.assertEquals('ccccccc', firstletters[:7])
+        self.assertFalse('c' in firstletters[7:], firstletters)
 
     def testUseMostFrequentlyAppearingWordForFields(self):
         puch_words_with_p = [p for p in PUCH_WORDS if p.lower().startswith('p')]
@@ -175,10 +173,8 @@
             self.addWord(word)
             self.addWord(word, 'field0')
         inclusive, suggestions = self.suggestionsFor('puch', fieldname='field0')
-        threeletterwords = [word[:3] for word in suggestions]
-        self.assertTrue('puc' not in threeletterwords[:5])
-        self.assertEquals(['puc']*20, threeletterwords[5:])
 
+        self.assertEquals(['punch', 'pampuch', 'pouch', 'puck', 'pupuluca', 'puccini', 'praepuce', 'prepuce', 'prepuces', 'puchera'], suggestions[:10])
 
     def testNGramForSpecificField(self):
         for i in range(3):
